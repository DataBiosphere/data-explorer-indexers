## GCS indexer

### Quickstart

Index [Platinum Genomes GCS files](https://pantheon.corp.google.com/storage/browser/genomics-public-data/platinum-genomes)
into a local Elasticsearch container.

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* From `gcs` directory, run: `docker-compose up --build`
* View Elasticsearch index:
  ```
  http://localhost:9200/_cat/indices?v
  http://localhost:9200/platinum_genomes/_search?pretty=true
  ```

If you want to run the Data Explorer UI on this dataset, follow the instructions
below. Note that you will have to reindex the data into an Elasticsearch
container from the [data-explorer repo](https://github.com/DataBiosphere/data-explorer/).

### Index a custom dataset locally

We assume that primary key appears in the GCS file path. For example, the file
`gs://genomics-public-data/platinum-genomes/bam/NA12890_S1.bam` has primary key
`NA12890`. Primary key can appear anywhere in the path; being part of a
directory or file name is fine.
[See here](https://github.com/DataBiosphere/data-explorer-indexers#overview)
for background on primary key,

If your GCS files don't follow this convention -- for example, if the file paths
contain sample ID instead of participant ID -- please file an Issue and we will
add support for this.

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Setup config files.
  * Create `dataset_config/<my dataset>`. Copy `dataset_config/template/*` to this directory.
  * Edit config files; instructions are in the files
* Run Elasticsearch:
  * If you intend to run the [Data Explorer UI](https://github.com/DataBiosphere/data-explorer/)
  after this, run inside the *data-explorer* repo:
    ```
    docker-compose up -d elasticsearch
    ```
  * If you do not intend to run the Data Explorer UI after this, and just want
  to inspect the index in Elasticsearch, run inside this repo from the
  `gcs` directory:
    ```
    docker-compose up -d elasticsearch
    ```
* Run the indexer. From `gcs` directory, run:
  ```
  DATASET_CONFIG_DIR=dataset_config/<my dataset> docker-compose up --build indexer
  ```
* View Elasticsearch index:
  ```
  http://localhost:9200/_cat/indices?v
  http://localhost:9200/MY_DATASET/_search?pretty=true
  ```
* Optionally, [bring up a local Data Explorer UI](https://github.com/DataBiosphere/data-explorer/blob/5441559c57ab7a2e0813e8e4fe7e19a9394f1bdf/README.md#run-local-data-explorer-with-a-specific-dataset).

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run from `gcs` directory:

```
virtualenv ~/virtualenv/indexer-gcs
source ~/virtualenv/indexer-gcs/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f | sed 's/^indexer-util.*/\.\/indexer_util/g' > requirements.txt
deactivate
```
