## BigQuery indexer

### Quickstart

Index the public [1000 Genomes BigQuery table](https://bigquery.cloud.google.com/table/genomics-public-data:1000_genomes.sample_info)
into a local Elasticsearch container.

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Determine the project that will be billed for querying the BigQuery tables.
Your account must have `bigquery.jobs.create` permission on this project; this
includes any project where you have the Viewer/Editor/Owner role.
* If `docker network ls` doesn't show `data-explorer_default`, run:
`docker network create data-explorer_default`
* From `bigquery` directory, run:
 `BILLING_PROJECT_ID=<billing project id> docker-compose up --build`
* View Elasticsearch index:
  ```
  http://localhost:9200/_cat/indices?v
  http://localhost:9200/1000_genomes/_search?pretty=true
  ```

If you want to run the Data Explorer UI on this dataset, follow the instructions
below. Note that you will have to reindex the data into an Elasticsearch
container from the [data-explorer repo](https://github.com/DataBiosphere/data-explorer/).

### Index a custom dataset locally

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Setup config files.
  * Create `dataset_config/<my dataset>`. Copy `dataset_config/template/*` to this directory.
  * Edit config files; instructions are in the files. Read
  [Overview](https://github.com/DataBiosphere/data-explorer-indexers#overview)
  for some background information.
* Run Elasticsearch:
  * If you intend to run the [Data Explorer UI](https://github.com/DataBiosphere/data-explorer/)
  after this, run inside the *data-explorer* repo:
    ```
    docker-compose up -d elasticsearch
    ```
  * If you do not intend to run the Data Explorer UI after this, and just want
  to inspect the index in Elasticsearch, run inside this repo from the
  `bigquery` directory:
    ```
    docker-compose up -d elasticsearch
    ```
* Determine the project that will be billed for querying the BigQuery tables.
Your account must have `bigquery.jobs.create` permission on this project; this
includes any project where you have the Viewer/Editor/Owner role.
* Run the indexer. From `bigquery` directory, run:
  ```
  BILLING_PROJECT_ID=<billing project id> DATASET_CONFIG_DIR=dataset_config/<my dataset> docker-compose up --build indexer
  ```
* View Elasticsearch index:
  ```
  http://localhost:9200/_cat/indices?v
  http://localhost:9200/MY_DATASET/_search?pretty=true
  ```
* Optionally, [bring up a local Data Explorer UI](https://github.com/DataBiosphere/data-explorer/blob/5441559c57ab7a2e0813e8e4fe7e19a9394f1bdf/README.md#run-local-data-explorer-with-a-specific-dataset).

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run from `bigquery` directory:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f | sed 's/^indexer-util.*/\.\/indexer_util/g' > requirements.txt
deactivate
```

### Troubleshooting

When indexing a large table on a Mac, Elasticsearch may crash with no error
message in the logs. Try increasing Docker's memory, for example from 2G to 3G.
