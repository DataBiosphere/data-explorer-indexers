## BigQuery indexer

### Quickstart

Index the default
[public BigQuery table](https://bigquery.cloud.google.com/table/google.com:biggene:platinum_genomes.sample_info) 
into a local Elasticsearch container.

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Determine the project that will be billed for querying the BigQuery tables.
Your account must have `bigquery.jobs.create` permission on this project; this
includes any project where you have the Viewer/Editor/Owner role.
* From `bigquery` directory, run:
 `BILLING_PROJECT_ID=<billing project id> docker-compose up --build`
* View the Elasticsearch index:
`http://localhost:9200/platinum_genomes/_search?pretty=true`

If you want to run the Data Explorer UI on this dataset, follow the instructions
below. Note that you will have to reindex the data into an Elasticsearch
container from the [data-explorer repo](https://github.com/DataBiosphere/data-explorer/).

### Index a custom dataset locally

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Setup config files.
  * Create `dataset_config/<my dataset>`. Copy `dataset_config/template/*` to this directory.
  * Edit config files; instructions are in the files. Read
  [Overview](https://github.com/DataBiosphere/data-explorer-indexers/tree/master/bigquery#overview)
  for some background information.
* Run Elasticsearch:
  * If you intend to run the [Data Explorer UI](https://github.com/DataBiosphere/data-explorer/)
  after this, run inside the *data-explorer* repo:
    ```
    docker-compose up -d elasticsearch
    ```
  * If you do not intend to run the Data Explorer UI after this, and just want
  to inspect the index in Elasticsearch, run inside this repo from the
  `bigquery` directory:
    ```
    docker-compose up -d elasticsearch
    ```
* Determine the project that will be billed for querying the BigQuery tables.
Your account must have `bigquery.jobs.create` permission on this project; this
includes any project where you have the Viewer/Editor/Owner role.
* Run the indexer. From `bigquery` directory, run:
  ```
  BILLING_PROJECT_ID=<billing project id> DATASET_CONFIG_DIR=dataset_config/<my dataset> docker-compose up --build indexer
  ```
* List Elasticsearch indices: `http://localhost:9200/_cat/indices?v`  
  View the Elasticsearch index:
  `http://localhost:9200/MY_DATASET/_search?pretty=true`
* Optionally, [bring up a local Data Explorer UI](https://github.com/DataBiosphere/data-explorer/blob/5441559c57ab7a2e0813e8e4fe7e19a9394f1bdf/README.md#run-local-data-explorer-with-a-specific-dataset).

### Overview

A Data explorer UI allows for faceted search. For example,
[Boardwalk](https://ucsc-cgp.org/boardwalk) has facets: Analysis Type, Center
Name, etc. A dataset may have hundreds of fields (weight, height, etc); the
dataset owner must curate a list of facets that they think will be interesting.
For [Project Baseline](https://www.projectbaseline.com/), facets may include
age, weight, height, etc.

A Dataset has a notion of `primary_key`. For a dataset that tracks 1000
participants, `primary_key` could be `participant_id`. For a dataset that
contains 1000 samples from a single person, `primary_key` could be `sample_id`.

`primary_key` is used to tie information together from different BigQuery
tables. Say there are facets for age and weight; age and weight are
stored in separate BigQuery tables; and `primary_key` is `participant_id`.
First, age table is indexed. An Elasticsearch document is created for each
`participant_id` and has document id = `participant_id`. A document would look
like:

```
{
  "age": "30",
}
```

Then, the weight table is indexed. The Elasticsearch documents will get a new
weight field:

```
{
  "age": "30",
  "weight": "140",
}
```

`participant_id` will be used to figure out which document to update.

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f | sed 's/^indexer-util.*/\.\.\/indexer_util/g' > requirements.txt
deactivate
```
