## BigQuery indexer

### Quickstart

Index a BigQuery table into an Elasticsearch container on your local machine.

* If you want to use a [sample public dataset](https://bigquery.cloud.google.com/table/google.com:biggene:platinum_genomes.sample_info):
  * [Install bq](https://cloud.google.com/bigquery/docs/bq-command-line-tool#installation)
if you haven't done so already.
  * Copy dataset to your project.
    ```
    bq --project_id MY_GOOGLE_CLOUD_PROJECT mk platinum_genomes
    bq --project_id MY_GOOGLE_CLOUD_PROJECT cp google.com:biggene:platinum_genomes.sample_info  MY_GOOGLE_CLOUD_PROJECT:platinum_genomes.sample_info
    ```
  * Change project ids in `dataset_config/platinum_genomes/facet_fields.csv`.
* If you want to use your own dataset:
  * Create a config directory for your dataset, e.g. `dataset_config/amp_pd`.
  Copy `dataset_config/template/*` to this directory.
  * Edit config files; instructions are in the files. Read
  [Overview](https://github.com/DataBiosphere/data-explorer-indexers/tree/master/bigquery#overview)
  for some background information.
* If `~/.config/gcloud/application_default_credentials.json` doesn't exist, create it by running `gcloud auth application-default login`.
* Run Elasticsearch.
  * If you intend to run the Data Explorer UI from the [data-explorer repo](https://github.com/DataBiosphere/data-explorer/)
    after this, run inside the data-explorer repo:
    ```
    docker-compose up --build elasticsearch
    ```
  * If you do not intend to run the Data Explorer UI after this, and just want
    to inspect the index in Elasticsearch, run:
    ```
    docker run -p 9200:9200 docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.2
    ```
    In `docker-compose.yml`, change ELASTICSEARCH_URL to `localhost:9200`.
* Run the indexer.
  * If using default dataset: `docker-compose up --build`
  * If using custom dataset: `DATASET_CONFIG_DIR=dataset_config/MY_DATASET docker-compose up --build`
* View Elasticsearch index at
 `http://localhost:9200/platinum_genomes/_search?pretty=true`. If using your
 own dataset, change `platinum_genomes` to your dataset name.

If using your own dataset: Now that your dataset is indexed, follow
https://github.com/DataBiosphere/data-explorer to bring up a Data explorer UI.

### Overview

A Data explorer UI allows for faceted search. For example,
[Boardwalk](https://ucsc-cgp.org/boardwalk) has facets: Analysis Type, Center
Name, etc. A dataset may have hundreds of fields (weight, height, etc); the
dataset owner must curate a list of facets that they think will be interesting.
For [Project Baseline](https://www.projectbaseline.com/), facets may include
age, weight, height, etc.

A Dataset has a notion of `primary_key`. For a dataset that tracks 1000
participants, `primary_key` could be `participant_id`. For a dataset that
contains 1000 samples from a single person, `primary_key` could be `sample_id`.

`primary_key` is used to tie information together from different BigQuery
tables. Say there are facets for age and weight; age and weight are
stored in separate BigQuery tables; and `primary_key` is `participant_id`.
First, age table is indexed. An Elasticsearch document is created for each
`participant_id` and has document id = `participant_id`. A document would look
like:

```
{
  "age": "30",
}
```

Then, the weight table is indexed. The Elasticsearch documents will get a new
weight field:

```
{
  "age": "30",
  "weight": "140",
}
```

`participant_id` will be used to figure out which document to update.

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f > requirements.txt
deactivate
```
