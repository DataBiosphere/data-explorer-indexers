## BigQuery indexer

Follow the instructions below to create an Elasticsearch index in an
Elasticsearch docker container. Note that we run indexer.py directly (not using
docker) because it's trickier to authenticate to Google Cloud Platform from
within docker.

### Quickstart

Index a BigQuery table into an Elasticsearch container on your local machine.

* If you want to use a [sample public dataset](https://bigquery.cloud.google.com/table/google.com:biggene:platinum_genomes.sample_info):
  * [Install bq](https://cloud.google.com/bigquery/docs/bq-command-line-tool#installation)
if you haven't done so already.
  * Copy dataset to your project.
    ```
    bq --project_id MY_GOOGLE_CLOUD_PROJECT mk platinum_genomes
    bq --project_id MY_GOOGLE_CLOUD_PROJECT cp google.com:biggene:platinum_genomes.sample_info  MY_GOOGLE_CLOUD_PROJECT:platinum_genomes.sample_info
    ```
  * Change project ids in `config/platinum_genomes/facet_fields.csv`.
* If you want to use your own dataset:
  * Create a `config/private` directory. (Files in `config/private` won't be
added to this git repo.) Make a new directory under `config/private` and copy
`config/template/*` to it.
  * Edit config files; instructions are in the files. Read
  [Overview](https://github.com/DataBiosphere/data-explorer-indexers/tree/master/bigquery#overview)
  for some background information.
* If `~/.config/gcloud/application_default_credentials.json` doesn't exist, create it by running `gcloud auth application-default login`.
* If using default dataset: `docker-compose up --build`  
  If using custom dataset: `DATASET_CONFIG_DIR=config/private/MY_DATASET docker-compose up --build`
* View Elasticsearch index at
 `http://localhost:9200/platinum_genomes/_search?pretty=true`. If using your
 own dataset, change `platinum_genomes` to your dataset name.

### Overview

A Data explorer UI allows for faceted search. For example,
[Boardwalk](https://ucsc-cgp.org/boardwalk) has facets: Analysis Type, Center
Name, etc. A dataset may have hundreds of fields (weight, height, etc); the
dataset owner must curate a list of facets that they think will be interesting.
For [Project Baseline](https://www.projectbaseline.com/), facets may include
age, weight, height, etc.

To set up Data explorer for a new dataset:

* Setup `config` directory. This includes specifying facet fields.
* Run indexer to index BigQuery tables into Elasticsearch.
* Using https://github.com/DataBiosphere/data-explorer repo, run Data explorer
and point to `config` directory from first step.

A Dataset has a notion of `primary_key`. For a dataset that tracks 1000
participants, `primary_key` could be `participant_id`. For a dataset that
contains 1000 samples from a single person, `primary_key` could be `sample_id`.

`primary_key` is used to tie information together from different BigQuery
tables. Say there are facets for age and weight; age and weight are
stored in separate BigQuery tables; and `primary_key` is `participant_id`.
First, age table is indexed. An Elasticsearch document is created for each
`participant_id` and has document id = `participant_id`. A document would look
like:

```
{
  "age": "30",
}
```

Then, the weight table is indexed. The Elasticsearch documents will get a new
weight field:

```
{
  "age": "30",
  "weight": "140",
}
```

`participant_id` will be used to figure out which document to update.

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f > requirements.txt
deactivate
```
