## BigQuery indexer

We use docker to run Elasticsearch. We run indexer.py directly (not using
docker) because it's trickier to authenticate to Google Cloud Platform with
docker.

### Quickstart

* [Install bq](https://cloud.google.com/bigquery/docs/bq-command-line-tool#installation)
if you haven't done so already.
* Copy a simple public dataset to your project.
    ```
    bq --project_id <myproject> mk platinum_genomes
    bq --project_id <myproject> cp google.com:biggene:platinum_genomes.sample_info  <myproject>:platinum_genomes.sample_info
    ```
* Change project names in `config/platinum_genomes/facet_fields.csv`.
* Run Elasticsearch.

    ```
    docker run -p 9200:9200 docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.2
    ```
* Run indexer.

    ```
    virtualenv ~/virtualenv/indexer-bigquery
    source ~/virtualenv/indexer-bigquery/bin/activate
    pip install requirements.txt
    python indexer.py
    ```

* View Elasticsearch index at
 `http://localhost:9200/platinum_genomes/_search?pretty=true`.

### Overview

Index BigQuery tables into Elasticsearch. Only specified facet fields will be indexed.

Given this BigQuery table:

  participant_id | age | weight
  --- | --- |---
  1 | 23 | 140
  2 | 33 | 150

Elasticsearch index will contain 2 documents. First document has id `1` and contains:

	{
	  "age": "23",
	  "weight": "140",
	}

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f > requirements.txt
deactivate
```
