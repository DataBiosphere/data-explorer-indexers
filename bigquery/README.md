## BigQuery indexer

Run Elasticsearch via docker. We run indexer.py directly (not using
docker) because it's trickier to authenticate to Google Cloud Platform from
within docker.

### Quickstart

* If you want to use a [sample public dataset](https://bigquery.cloud.google.com/table/google.com:biggene:platinum_genomes.sample_info):
  * [Install bq](https://cloud.google.com/bigquery/docs/bq-command-line-tool#installation)
if you haven't done so already.
  * [Install virtualenv](https://pypi.python.org/pypi/virtualenv).
  * Copy dataset to your project.
    ```
    bq --project_id <myproject> mk platinum_genomes
    bq --project_id <myproject> cp google.com:biggene:platinum_genomes.sample_info  <myproject>:platinum_genomes.sample_info
    ```
  * Change project ids in `config/platinum_genomes/facet_fields.csv`.
* If you want to use your own dataset:
  * Make a new directory under `config` and copy `config/template/*` to it.
  * Edit config files; instructions are in the files.
* Run Elasticsearch.

    ```
    docker run -p 9200:9200 docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.2
    ```
* Run indexer.

    ```
    virtualenv ~/virtualenv/indexer-bigquery
    source ~/virtualenv/indexer-bigquery/bin/activate
    pip install requirements.txt
    python indexer.py                            # If using default dataset
    python indexer.py --config_dir <config_dir>  # If using your own dataset
    ```

* View Elasticsearch index at
 `http://localhost:9200/platinum_genomes/_search?pretty=true`. If using your
 own dataset, change `platinum_genomes` to your dataset name.

### Overview

Index BigQuery tables into Elasticsearch. Only specified facet fields will be indexed.

Given this BigQuery table:

  participant_id | age | weight
  --- | --- |---
  1 | 23 | 140
  2 | 33 | 150

Elasticsearch index will contain 2 documents. First document has id `1` and contains:

	{
	  "age": "23",
	  "weight": "140",
	}

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f > requirements.txt
deactivate
```

### Running on GKE

For a Kubernetes cluster running an externally or internally exposed elasticsearch
instance, the bigquery indexer can be run as a Kubernetes job as follows:

1. Build, tag, and upload the base docker image to GCR:
    ```
    docker build -t gcr.io/PROJECT_ID/es-indexer-base .
    docker push gcr.io/PROJECT_ID/es-indexer-base:latest
    ```
2. Pull this repository into a properly configured and authenticated console (e.g. GCP
console) and update `indexer-job.yaml` with the desired MY_GOOGLE_CLOUD_PROJECT,
LOAD_BALANCER_IP and CONFIG_DIR.
3. [Using steps 3-5 as a reference,](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform#step_3_create_service_account_credentials)
create a secret named `indexer-key` for the Compute Engine service account private
key so that the service account can access BigQuery.
4. Generate a configmap for the indexer:
    ```
    kubectl create configmap indexer --from-file=indexer.py
    ```
5. Run the job:
    ```
    kubectl create -f indexer-job.yaml
    ```
6. Verify the indexer was successful:
    ```
    kubectl get pods -a
    kubectl logs bq-indexer-xxxxx
    ```