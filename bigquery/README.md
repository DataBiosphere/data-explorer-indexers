## BigQuery indexer

Follow the instructions below to create an Elasticsearch index in an
Elasticsearch docker container. Note that we run indexer.py directly (not using
docker) because it's trickier to authenticate to Google Cloud Platform from
within docker.

### Quickstart

* If you want to use a [sample public dataset](https://bigquery.cloud.google.com/table/google.com:biggene:platinum_genomes.sample_info):
  * [Install bq](https://cloud.google.com/bigquery/docs/bq-command-line-tool#installation)
if you haven't done so already.
  * Copy dataset to your project.
    ```
    bq --project_id <myproject> mk platinum_genomes
    bq --project_id <myproject> cp google.com:biggene:platinum_genomes.sample_info  <myproject>:platinum_genomes.sample_info
    ```
  * Change project ids in `config/platinum_genomes/facet_fields.csv`.
* If you want to use your own dataset:
  * Create a `config/private` directory. (Files in `config/private` won't be
added to this git repo.) Make a new directory under `config/private` and copy
`config/template/*` to it.
  * Edit config files; instructions are in the files. Read
  [Overview](https://github.com/DataBiosphere/data-explorer-indexers/tree/master/bigquery#overview)
  for some background information.
* Run Elasticsearch.

    ```
    docker run -p 9200:9200 docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.2
    ```
* Run indexer.
  * [Install virtualenv](https://pypi.python.org/pypi/virtualenv) if you haven't done so already.

    ```
    virtualenv ~/virtualenv/indexer-bigquery
    source ~/virtualenv/indexer-bigquery/bin/activate
    pip install -r requirements.txt
    python indexer.py                            # If using default dataset
    python indexer.py --config_dir <config_dir>  # If using your own dataset
    ```

* View Elasticsearch index at
 `http://localhost:9200/platinum_genomes/_search?pretty=true`. If using your
 own dataset, change `platinum_genomes` to your dataset name.

### Overview

A Data explorer UI allows for faceted search. For example,
[Boardwalk](https://ucsc-cgp.org/boardwalk) has facets: Analysis Type, Center
Name, etc. A dataset may have hundreds of fields (weight, height, etc); the
dataset owner must curate a list of facets that they think will be interesting.
For [Project Baseline](https://www.projectbaseline.com/), facets may include
age, weight, height, etc.

To set up Data explorer for a new dataset:

* Setup `config` directory. This includes specifying facet fields.
* Run indexer to index BigQuery tables into Elasticsearch.
* Using https://github.com/DataBiosphere/data-explorer repo, run Data explorer
and point to `config` directory from first step.

A Dataset has a notion of `primary_key`. For a dataset that tracks 1000
participants, `primary_key` could be `participant_id`. For a dataset that
contains 1000 samples from a single person, `primary_key` could be `sample_id`.

`primary_key` is used to tie information together from different BigQuery
tables. Say there are facets for age and weight; age and weight are
stored in separate BigQuery tables; and `primary_key` is `participant_id`.
First, age table is indexed. An Elasticsearch document is created for each
`participant_id` and has document id = `participant_id`. A document would look
like:

```
{
  "age": "30",
}
```

Then, the weight table is indexed. The Elasticsearch documents will get a new
weight field:

```
{
  "age": "30",
  "weight": "140",
}
```

`participant_id` will be used to figure out which document to update.

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f > requirements.txt
deactivate
```

### Running on GKE

For a Kubernetes cluster running an externally or internally exposed elasticsearch
instance, the bigquery indexer can be run as a Kubernetes job as follows:

1. Build, tag, and upload the base docker image to GCR:
    ```
    docker build -t gcr.io/PROJECT_ID/es-indexer-base .
    docker push gcr.io/PROJECT_ID/es-indexer-base:latest
    ```
2. Pull this repository into a properly configured and authenticated console (e.g. GCP
console) and update `indexer-job.yaml` with the desired MY_GOOGLE_CLOUD_PROJECT,
LOAD_BALANCER_IP and CONFIG_DIR.
3. [Using steps 3-5 as a reference,](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform#step_3_create_service_account_credentials)
create a secret named `indexer-key` for the Compute Engine service account private
key so that the service account can access BigQuery.
4. Generate a configmap for the indexer:
    ```
    kubectl create configmap indexer --from-file=indexer.py
    ```
5. Run the job:
    ```
    kubectl create -f indexer-job.yaml
    ```
6. Verify the indexer was successful:
    ```
    kubectl get pods -a
    kubectl logs bq-indexer-xxxxx
    ```