## BigQuery indexer

### Quickstart

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Providing a GCP Project ID that you can use as the billing project, run the
indexer:
  `BILLING_PROJECT_ID=<billing_project_id> docker-compose up --build`
* View Elasticsearch index at
 `http://localhost:9200/platinum_genomes/_search?pretty=true`.
 
### Stand up your own dataset

* If `~/.config/gcloud/application_default_credentials.json` doesn't exist,
create it by running `gcloud auth application-default login`.
* Run Elasticsearch:
  * If you intend to run the [Data Explorer UI](https://github.com/DataBiosphere/data-explorer/)
  after this, run inside the data-explorer repo:
    ```
    docker-compose up -d elasticsearch
    ```
  * If you do not intend to run the Data Explorer UI after this, and just want
  to inspect the index in Elasticsearch, run inside this repo:
    ```
    docker-compose up -d elasticsearch
    ```
* Create a config directory for your dataset, e.g. `dataset_config/MY_DATASET`.
Copy `dataset_config/template/*` to this directory.
* Edit config files; instructions are in the files. See [Overview](https://github.com/DataBiosphere/data-explorer-indexers/tree/master/bigquery#overview)
for some background information.
* Determine the project that will be billed for querying the BigQuery tables.
You must have `bigquery.jobs.create` on this project; for example, any project
where you are Viewer/Editor/Owner.
* Run the indexer:
  ```
  BILLING_PROJECT_ID=<billing_project_id> DATASET_CONFIG_DIR=dataset_config/MY_DATASET docker-compose up --build indexer
  ```
* View Elasticsearch index at
 `http://localhost:9200/MY_DATASET_NAME/_search?pretty=true`. 
* Optionally, [bring up a Data Explorer UI](https://github.com/DataBiosphere/data-explorer).

### Overview

A Data explorer UI allows for faceted search. For example,
[Boardwalk](https://ucsc-cgp.org/boardwalk) has facets: Analysis Type, Center
Name, etc. A dataset may have hundreds of fields (weight, height, etc); the
dataset owner must curate a list of facets that they think will be interesting.
For [Project Baseline](https://www.projectbaseline.com/), facets may include
age, weight, height, etc.

A Dataset has a notion of `primary_key`. For a dataset that tracks 1000
participants, `primary_key` could be `participant_id`. For a dataset that
contains 1000 samples from a single person, `primary_key` could be `sample_id`.

`primary_key` is used to tie information together from different BigQuery
tables. Say there are facets for age and weight; age and weight are
stored in separate BigQuery tables; and `primary_key` is `participant_id`.
First, age table is indexed. An Elasticsearch document is created for each
`participant_id` and has document id = `participant_id`. A document would look
like:

```
{
  "age": "30",
}
```

Then, the weight table is indexed. The Elasticsearch documents will get a new
weight field:

```
{
  "age": "30",
  "weight": "140",
}
```

`participant_id` will be used to figure out which document to update.

### Generating `requirements.txt`

`requirements.txt` is autogenerated from `requirements-to-freeze.txt`. The
latter lists only direct dependencies. To regenerate run:

```
virtualenv ~/virtualenv/indexer-bigquery
source ~/virtualenv/indexer-bigquery/bin/activate
pip install -r requirements-to-freeze.txt
pip freeze | sort -f > requirements.txt
deactivate
```
