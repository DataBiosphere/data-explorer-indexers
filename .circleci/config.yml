version: 2
jobs:
  build:
    machine:
      image: circleci/classic:latest
    environment:
      GOOGLE_PROJECT_ID: ci-test-data-explorer
    steps:
      - checkout
      - restore_cache:
          key: virtualenv-{{ .Branch }}-{{ checksum "bigquery/requirements.txt" }}
      - run:
          name: Run the yapf python linter
          command: |
            pip install yapf
            yapf -dr .
      - run:
          name: Run the BigQuery Indexer
          command: |
            echo ${GOOGLE_SERVICE_KEY} | base64 --decode > ${HOME}/.config/gcloud/application_default_credentials.json
            docker network create data-explorer_default
            cd bigquery
            # Run Elasticsearch in the background with the indexer in the foreground to prevent blocking the main thread
            docker-compose up -d elasticsearch
            BILLING_PROJECT_ID=${GOOGLE_PROJECT_ID} docker-compose up --build indexer
            # For some reason index isn't available right after indexer terminates, so sleep
            sleep 5
            # Assert the 1000_genomes index has 3500 documents
            expr $(curl -s 'http://localhost:9200/1000_genomes/_search' | jq -r '.hits.total') = "3500"
            curl -XDELETE localhost:9200/1000_genomes
            docker-compose stop
      - run:
          name: Run the GCS Indexer
          command: |
            echo ${GOOGLE_SERVICE_KEY} | base64 --decode > ${HOME}/.config/gcloud/application_default_credentials.json
            # Note: data-explorer_default has already been created
            cd gcs
            # Run Elasticsearch in the background with the indexer in the foreground to prevent blocking the main thread
            docker-compose up -d elasticsearch
            BILLING_PROJECT_ID=${GOOGLE_PROJECT_ID} docker-compose up --build indexer
            # For some reason index isn't available right after indexer terminates, so sleep
            sleep 5
            # Assert the 1000_genomes index has 3500 documents
            expr $(curl -s 'http://localhost:9200/1000_genomes/_search' | jq -r '.hits.total') = "2559"
            curl -XDELETE localhost:9200/1000_genomes
            docker-compose stop
      - save_cache:
          key: virtualenv-{{ .Branch }}-{{ checksum "bigquery/requirements.txt" }}
          paths:
            - "~/yapf"
